\documentclass[12pt]{article}

%\usepackage[T1]{fontenc} % Support accents copy-pasting
\usepackage[utf8]{inputenc} % Input encoding
\usepackage[french]{babel}
\usepackage[a4paper,width=15cm,top=2.5cm,bottom=2.5cm]{geometry} % Size of the page

\usepackage[ruled]{algorithm2e} % Algorithms
\usepackage{float} % H option
\usepackage{listings} % Typeset programs within the document
\usepackage{array,tabularx} % Extends tabular
\usepackage{amsmath,amsthm,amsfonts,amssymb} % Math
\usepackage{braket} % For \Set
\usepackage{caption} % Caption options
\usepackage{subcaption} % For subfigures
\usepackage{graphicx} % For \includegraphics
\usepackage[export]{adjustbox} % Extends \includegraphics
\usepackage{tikz}
\PassOptionsToPackage{hyphens}{url} % Break urls at new lines
\usepackage{hyperref} % For hyperlinks
\usepackage{url} % For \url
\usepackage{svg} % To include svg images
\usepackage{mathtools} % for KL divergence

% Tikz options
\usetikzlibrary{positioning}
\usetikzlibrary{trees}
\usetikzlibrary{arrows,automata}
\usetikzlibrary{positioning}
\usetikzlibrary{decorations.pathmorphing}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{calc}
\usetikzlibrary{snakes}
\usetikzlibrary{shapes.arrows}

% Hyperlinks colors
\hypersetup{
	colorlinks,
	linkcolor={red!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}

% Numerotation depth in the document and in the table of contents
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}

% algorithm2e options
\SetEndCharOfAlgoLine{}

% Command to specify the source of a figure on a new line
\newcommand{\source}[1]{\vspace{-5pt} \caption*{ Source: {#1}} }

% Command to number an equation in align* environment
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

% Color commands
\newcommand{\red}[1]{{\color{red}{#1}}}
\newcommand{\blue}[1]{{\color{blue}{#1}}}

% Math commands
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Mod}{\ \mathrm{mod}\ }
\newcommand*{\pd}[3][]{\ensuremath{\frac{\partial^{#1} #2}{\partial #3}}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\EE}[2]{\mathbb{E}_{#1\!\!}\left[#2\right]}
\newcommand{\CEE}[3]{\EE{#1}{{#2}~\middle\vert~{#3}}}
\def\E#1{\EE{\,}{#1}}
\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
	#1\;\delimsize\|\;#2%
}
\newcommand{\infdiv}{D\infdivx}
\def\Pr#1{{{\rm Pr\!}\left\{#1\right\}}}

% Theorems
\newtheorem{theorem}{Théorème}
\newtheorem{definition}{Définition}

\title{Projet d'Advanced Machine Learning : AdaBoost}
\date{\today}
\author{Charly Delfosse, Arnaud Palgen, Victor Dheur}

\begin{document}
	\maketitle
	
	\section{Principe du boosting}
	
	Le boosting est une méthode permettant d'augmenter les performances\\ d'algorithmes apprenants
	faibles (voir def.~\ref{def:weaklearner}). Cette méthode permet aussi de résoudre deux problèmes
	rencontrés lors de l'apprentissage:
	\begin{enumerate}
		\item Le tradeoff biais-complexité
		\item La complexité des calculs
	\end{enumerate}
	Le principe général du boosting consiste à commencer par une hypothèse de base,
	qui est ajustée à chaque itération de l'algorithme pour produire une hypothèse plus précise.
	
	
	\begin{definition}{Algorithme $\gamma$ apprenant faible}
		
		Un algorithme \textit{A} est $\gamma$ apprenant faible (weak learner) pour une classe
		$\mathcal{H}$ s'il existe une fonction $m_{\mathcal{H}}: (0,1) \rightarrow \mathbb{N}$ tel que 
		pour tout $\delta \in (0,1)$, pour toute distribution $\mathcal{D}$ sur $\mathcal{X}$ et pour
		chaque fonction de labelisation $f:\mathcal{X} \rightarrow \{\pm 1\}$, si l'hypothèse est
		valable pour $\mathcal{H}$, $\mathcal{D}$, $\mathcal{F}$, alors lors de l'execution de l'algorithme
		d'apprentissage sur $m > m_{\mathcal{H}}(\delta)$ i.i.d exemples générés par $\mathcal{D}$ et
		labelisés par $f$, l'algorithme retourne, avec une probabilité $1- \delta$, une hypotèse
		\textit{h} tel que $L_{(\mathcal{D}, f)}(H) \leq \frac{1}{2} - \gamma$
		\label{def:weaklearner}
	\end{definition}
	
	\section{Méthode AdaBoost}
	
	\section{Bornes sur l’erreur de généralisation}
	
	On montre maintenant comment obtenir des bornes sur l'erreur de généralisation d'AdaBoost.
	Nous commençons par obtenir la dimension VC d'AdaBoost, puis nous appliquons l'inégalité VC.
	Cette approche est tirée du livre \cite{Shalev-Shwartz2014-ba}, et des détails supplémentaires ont été ajoutés.
	
	Nous avons vu que la sortie de l'algorithme AdaBoost est une hypothèse composée d'une combinaison linéaire d'hypothèses faibles.
	AdaBoost se base sur un weak-learner dont l'espace d'hypothèses est dénoté $B$.
	$T$ hypothèses $h_1, ..., h_T$ sont créées par ce weak-learner.
	La sortie d'AdaBoost fait partie de cet ensemble d'hypothèses :
	\[
	L(B, T) = \Set{ x \mapsto \text{sign}\left( \sum_{t=1}^T w_t h_t(x) \right) \mid \forall t, w_t \in \R \land h_t \in B}.
	\]
	
	Pour un espace d'hypothèses $\mathcal{H}$, nous dénotons $d_{VC}(\mathcal{H})$ sa dimension VC et $m_{\mathcal{H}}$ sa growth function.
	
	\begin{theorem}{(Dimension VC d'AdaBoost)}
		
		Nous allons montrer que, lorsque $T \geq 3$ et $d_{VC}(B) \geq 3$ :
		\[
		d_{VC}(L(B, T)) \leq T(d_{VC}(B) + 1) (3 \ln(T (d_{VC}(B) + 1)) + 2).
		\]
	\end{theorem}

	\begin{proof}
	
	Dénotons $d = d_{VC}(B)$ et supposons que $T \geq 3$ et $d_{VC}(B) \geq 3$.
	Soit $C = (x_1, ..., x_m)$ une séquence de points qui est shattered par $L(B, T)$.
	La création d'un labeling de $C$ par une hypothèse $h \in L(B, T)$ se fait en 2 étapes.
	D'abord, $T$ hypothèses $h_1, ..., h_T \in B$ sont sélectionnées par le weak-learner.
	Ensuite, un vecteur $w \in \R^T$ permet de créer la combinaison linéaire $\sum_{t=1}^T w_t h_t(x)$ pour un point $x$.
	On obtient ainsi un labeling $(h(x_1), ..., h(x_m))$ de $C$.
	
	Nous allons utiliser le lemme de Sauer, qui permet de borner supérieurement la growth function $m_{\mathcal{H}}$ d'un espace d'hypothèses $\mathcal{H}$ en utilisant la VC dimension $d_{VC}(\mathcal{H})$ :
	\[
	m_{\mathcal{H}}(m) \leq \left( \frac{em}{d_{VC}(\mathcal{H})} \right) ^{d_{VC}(\mathcal{H})}.
	\]
	
	Par le lemme de Sauer, au plus $\left( \frac{em}{d} \right)^{d}$ labelings différents de $C$ peuvent être créés à partir de l'espace d'hypothèses $B$.
	De plus, $T$ hypothèses qui créent ces labelings doivent être choisies, ce qui donne au plus $\left( \frac{em}{d} \right) ^{d T}$ labelings différents.
	En utilisant encore le lemme de Sauer, puisque la dimension VC d'un perceptron (sans biais) dans $\R^T$ est de $T$, la combinaison linéaire entraîne $\left( \frac{em}{T} \right)^{T}$ labelings différents.
	Nous avons donc :
	\[
	m_{L(B, T)}(m) \leq \left( \frac{em}{d} \right) ^{d T} \left( \frac{em}{T} \right)^{T}.
	\]
	
	En utilisant les hypothèses que $T \geq 3$ et $d_{VC}(B) \geq 3$, nous avons :
	\[
	\left( \frac{em}{d} \right) ^{d T} \left( \frac{em}{T} \right)^{T} \leq m^{(d + 1) T}.
	\]
	
	Puisque $C$ est shattered par $L(B, T)$, $m_{L(B, T)}(m) = 2^m$.
	
	Nous avons donc :
	\[
	2^m = m_{L(B, T)}(m) \leq m^{(d + 1) T}
	\]
	
	En passant au log :
	\[
	m \leq \ln(m) \frac{(d + 1) T}{\ln(2)}
	\]
	
	Il est possible de montrer (voir \cite{Shalev-Shwartz2014-ba} p.419 lemme A.1) que
	\[
	\forall a > 0, x \leq a \ln(x) \implies x \leq 2 a \ln(a).
	\]
	
	On déduit une borne sur $m$ qu'on borne encore par une expression plus simple :
	\[
	m \leq \frac{2 (d + 1) T}{\ln(2)} \ln \frac{(d + 1) T}{\ln(2)} \leq (d + 1) T (3 \ln((d + 1) T) + 2).
	\]
	
	En d'autres termes, le nombre de points $m$ qui peuvent être shattered par $L(B, T)$ est borné supérieurement par une expression qui dépend de $d$ et $T$.
	Puisque la dimension VC correspond au nombre maximum de points qui peuvent être shattered, l'expression reste vraie lorsque $m = d_{VC}(L(B, T))$ :
	\[
	d_{VC}(L(B, T)) \leq m \leq (d + 1) T (3 \ln((d + 1) T) + 2).
	\]
	\end{proof}
	
	Il ne reste plus qu'à utiliser l'inégalité VC en bornant la growth function par le lemme de Sauer pour avoir une borne sur l'erreur de généralisation.
	Nous utilisons l'inégalité VC présentée dans \cite{Bousquet2003-oz} à la page 192.
	
	En supposant que la loss produit des valeurs bornées dans $[0, 1]$, pour toute précision $\epsilon > 0$, on obtient :
	\begin{align*}
	\mathbb{P}\left[ \sup_{h \in L(B, T)} (R(h) - R_m(h)) \geq \epsilon \right] &\leq 4 m_{L(B, T)}(2 m) e^{-m \epsilon^2 / 8} \\
	&\leq 4 \left( \frac{2 m e}{d_{VC}(L(B, T))} \right)^{d_{VC}(L(B, T))} e^{-m \epsilon^2 / 8}.
	\end{align*}

	Un point important de ce développement est que la dimension VC de l'ensemble des hypothèse produites par AdaBoost augmente linéairement avec la dimension VC de $B$ et avec $T$, en ignorant les facteurs constants et logarithmiques.
	
	\bibliographystyle{alpha}
	\bibliography{report}
	
\end{document}